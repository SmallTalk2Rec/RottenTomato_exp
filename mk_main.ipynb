{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/visuworks/miniconda3/envs/chatbot_3.10.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/visuworks/.cache/kagglehub/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews/versions/4\n",
      "(143258, 16)\n",
      "(1444963, 11)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews\")\n",
    "path = path.replace('\\\\', '/')\n",
    "\n",
    "movie_df = pd.read_csv(os.path.join(path, os.listdir(path)[0]))\n",
    "review_df = pd.read_csv(os.path.join(path, os.listdir(path)[1]))\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(movie_df.shape)\n",
    "print(review_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of filtered_movie_ids: 7002\n",
      "# of filtered_review_ids: 7259\n"
     ]
    }
   ],
   "source": [
    "# 영화는 리뷰 개수가 상위 10%인 영화만 사용, 56개 이상\n",
    "tmp = review_df['id'].value_counts() >= 56\n",
    "filtered_movie_ids = tmp[tmp].index.tolist()\n",
    "\n",
    "# 사용자는 작성한 리뷰가 4개 이상인 사용자만 사용\n",
    "tmp = review_df['criticName'].value_counts() > 4\n",
    "filtered_review_ids = tmp[tmp].index.tolist()\n",
    "\n",
    "print(f\"# of filtered_movie_ids: {len(filtered_movie_ids)}\")\n",
    "print(f\"# of filtered_review_ids: {len(filtered_review_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_movie_ids를 set으로 변환하여 검색 속도 최적화\n",
    "filtered_movie_ids = set(filtered_movie_ids)\n",
    "filtered_review_ids = set(filtered_review_ids)\n",
    "\n",
    "filtered_review_df = review_df[review_df['id'].apply(lambda x: x in filtered_movie_ids)].reset_index(drop=True)\n",
    "filtered_review_df = filtered_review_df[filtered_review_df['criticName'].apply(lambda x: x in filtered_review_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_filtered_df():\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews\")\n",
    "    path = path.replace('\\\\', '/')\n",
    "\n",
    "    movie_df = pd.read_csv(os.path.join(path, os.listdir(path)[0]))\n",
    "    review_df = pd.read_csv(os.path.join(path, os.listdir(path)[1]))\n",
    "    \n",
    "    # 영화는 리뷰 개수가 상위 10%인 영화만 사용, 56개 이상\n",
    "    tmp = review_df['id'].value_counts() >= 56\n",
    "    filtered_movie_ids = tmp[tmp].index.tolist()\n",
    "\n",
    "    # 사용자는 작성한 리뷰가 4개 이상인 사용자만 사용\n",
    "    tmp = review_df['criticName'].value_counts() >= 4\n",
    "    filtered_review_ids = tmp[tmp].index.tolist()\n",
    "    \n",
    "    # filtered_movie_ids를 set으로 변환하여 검색 속도 최적화\n",
    "    filtered_movie_ids = set(filtered_movie_ids)\n",
    "    filtered_review_ids = set(filtered_review_ids)\n",
    "\n",
    "    filtered_review_df = review_df[review_df['id'].apply(lambda x: x in filtered_movie_ids)].reset_index(drop=True)\n",
    "    filtered_review_df = filtered_review_df[filtered_review_df['criticName'].apply(lambda x: x in filtered_review_ids)].reset_index(drop=True)\n",
    "    \n",
    "    scoreSentiment_map = {\"POSITIVE\": 1, \"NEGATIVE\": 0}\n",
    "\n",
    "    # map 메서드 사용\n",
    "    filtered_review_df['scoreSentiment'] = filtered_review_df['scoreSentiment'].map(scoreSentiment_map)\n",
    "\n",
    "    return filtered_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "(555578, 11)\n",
      "\n",
      "Validation DataFrame:\n",
      "(275831, 11)\n",
      "\n",
      "Test DataFrame:\n",
      "(99686, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def split_data(df, user_col, item_col, train_ratio=0.6, val_ratio=0.3, test_ratio=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train, validation, and test sets by a 6:3:1 ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing user and item interactions.\n",
    "    - user_col (str): The column name representing user IDs.\n",
    "    - item_col (str): The column name representing item (movie) IDs.\n",
    "    - train_ratio (float): Proportion of the dataset to include in the train split.\n",
    "    - val_ratio (float): Proportion of the dataset to include in the validation split.\n",
    "    - test_ratio (float): Proportion of the dataset to include in the test split.\n",
    "    - random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_df (pd.DataFrame): Training data.\n",
    "    - val_df (pd.DataFrame): Validation data.\n",
    "    - test_df (pd.DataFrame): Testing data.\n",
    "    \"\"\"\n",
    "    # Ensure the ratios add up to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum up to 1.\"\n",
    "\n",
    "    train_list, val_list, test_list = [], [], []\n",
    "\n",
    "    # Group by user to split data per user\n",
    "    for user, group in df.groupby(user_col):\n",
    "        # Shuffle the group\n",
    "        group = group.sample(frac=1, random_state=random_state)\n",
    "        n = len(group)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "        \n",
    "        # Split data\n",
    "        train_data = group.iloc[:train_end]\n",
    "        val_data = group.iloc[train_end:val_end]\n",
    "        test_data = group.iloc[val_end:]\n",
    "\n",
    "        # Append to respective lists\n",
    "        train_list.append(train_data)\n",
    "        val_list.append(val_data)\n",
    "        test_list.append(test_data)\n",
    "\n",
    "    # Concatenate all splits\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_list).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "filtered_review_df = get_filtered_df()\n",
    "train_df, val_df, test_df = split_data(filtered_review_df, user_col='criticName', item_col='id')\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df.shape)\n",
    "print(\"\\nValidation DataFrame:\")\n",
    "print(val_df.shape)\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 빌드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = \"criticName\"\n",
    "item_col = \"id\"\n",
    "label_col = \"scoreSentiment\"\n",
    "df = filtered_review_df\n",
    "\n",
    "num_movie_features = 64\n",
    "num_review_features = 64\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Create two node types \"paper\" and \"author\" holding a feature matrix:\n",
    "data['movie'].x = torch.randn(df[item_col].nunique(), num_movie_features)\n",
    "data['reviewer'].x = torch.randn(df[user_col].nunique(), num_review_features)\n",
    "\n",
    "# Create an edge type \"(author, writes, paper)\" and building the\n",
    "# graph connectivity:\n",
    "# Map user and item IDs to unique indices for graph processing\n",
    "unique_users = df[user_col].unique()\n",
    "unique_items = df[item_col].unique()\n",
    "\n",
    "user_map = {user: idx for idx, user in enumerate(unique_users)}\n",
    "item_map = {item: idx + len(unique_users) for idx, item in enumerate(unique_items)}\n",
    "\n",
    "# Convert edges to indices\n",
    "edge_index = []\n",
    "user_indices = df[user_col].map(user_map).values\n",
    "item_indices = df[item_col].map(item_map).values\n",
    "edge_index.append(torch.tensor([user_indices, item_indices], dtype=torch.long))\n",
    "    \n",
    "edge_index = torch.cat(edge_index, dim=1)  # Combine edges from train, val, test\n",
    "data['movie', 'rates', 'reviewer'].edge_index = edge_index  # [2, num_edges]\n",
    "data['movie', 'rates', 'reviewer'].rating = torch.tensor(df[label_col])  # float 타입\n",
    "\n",
    "data['reviewer', 'rates', 'movie'].edge_index = data['movie', 'rates', 'reviewer'].edge_index[[1, 0]]\n",
    "data['reviewer', 'rates', 'movie'].rating = torch.tensor(df[label_col])  # float 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = []\n",
    "tr_user_indices = train_df[user_col].map(user_map).values\n",
    "tr_item_indices = train_df[item_col].map(item_map).values\n",
    "edge_index.append(torch.tensor([tr_user_indices, tr_item_indices], dtype=torch.long))\n",
    "tr_edge_index = torch.cat(edge_index, dim=1)  # Combine edges from train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ...,  5513,  1273,   115],\n",
       "        [ 7494,  7494,  7494,  ..., 14495, 14495, 14495]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['movie', 'rates', 'reviewer'].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5870), tensor(9546))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_edge_index[0][0], tr_edge_index[1][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "rating = data['movie', 'rates', 'reviewer'].rating\n",
    "\n",
    "train_edge_index = torch.arange(int(0.6 * len(rating)))\n",
    "val_edge_index = torch.arange(int(0.6 * len(rating)), int(0.9 * len(rating)))\n",
    "test_edge_index = torch.arange(int(0.9 * len(rating)), len(rating))\n",
    "\n",
    "train_labels = data['movie', 'rates', 'reviewer'].rating[train_edge_index].float().to(device)\n",
    "val_labels = data['movie', 'rates', 'reviewer'].rating[val_edge_index].float().to(device)\n",
    "test_labels = data['movie', 'rates', 'reviewer'].rating[test_edge_index].float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      1,      2,  ..., 558654, 558655, 558656])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  movie={ x=[7002, 64] },\n",
       "  reviewer={ x=[7494, 64] },\n",
       "  (movie, rates, reviewer)={\n",
       "    edge_index=[2, 931095],\n",
       "    rating=[931095],\n",
       "  },\n",
       "  (reviewer, rates, movie)={\n",
       "    edge_index=[2, 931095],\n",
       "    rating=[931095],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "\n",
    "# HeteroGraphSAGE 정의\n",
    "class HeteroGraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super(HeteroGraphSAGE, self).__init__()\n",
    "        # 첫 번째 HeteroConv 레이어 정의\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('movie', 'rates', 'reviewer'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('reviewer', 'rates', 'movie'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # 두 번째 HeteroConv 레이어 정의\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('movie', 'rates', 'reviewer'): SAGEConv((hidden_channels, hidden_channels), out_channels),\n",
    "            ('reviewer', 'rates', 'movie'): SAGEConv((hidden_channels, hidden_channels), out_channels),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # 최종 분류 레이어 (이진 분류)\n",
    "        self.fc = Linear(out_channels, 1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # 첫 번째 GraphSAGE 레이어\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "\n",
    "        # 두 번째 GraphSAGE 레이어\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "\n",
    "        # 엣지 표현 계산 (user -> item)\n",
    "        user_item_edge_index = edge_index_dict[('movie', 'rates', 'reviewer')]\n",
    "        user_repr = x_dict['movie'][user_item_edge_index[0]]\n",
    "        item_repr = x_dict['reviewer'][user_item_edge_index[1]]\n",
    "\n",
    "        # 엣지별 예측값 계산\n",
    "        edge_repr = user_repr + item_repr\n",
    "        edge_prediction = torch.sigmoid(self.fc(edge_repr).squeeze(-1))  # Sigmoid for binary classification\n",
    "\n",
    "        return edge_prediction\n",
    "    \n",
    "    \n",
    "model = HeteroGraphSAGE(\n",
    "        metadata=data.metadata(),  # HeteroData의 메타데이터 사용\n",
    "        hidden_channels=128,\n",
    "        out_channels=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  movie={ x=[7002, 64] },\n",
       "  reviewer={ x=[7494, 64] },\n",
       "  (movie, rates, reviewer)={\n",
       "    edge_index=[2, 931095],\n",
       "    rating=[931095],\n",
       "  },\n",
       "  (reviewer, rates, movie)={\n",
       "    edge_index=[2, 931095],\n",
       "    rating=[931095],\n",
       "  },\n",
       "  (user, rates, item)={},\n",
       "  (user, rates, reviewer)={}\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroGraphSAGE(\n",
       "  (conv1): HeteroConv(num_relations=2)\n",
       "  (conv2): HeteroConv(num_relations=2)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 추출\n",
    "x_dict = data.x_dict\n",
    "edge_index_dict = data.edge_index_dict\n",
    "rating = data['movie', 'rates', 'reviewer'].rating.float().to(device)\n",
    "pred = model(x_dict, edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([931095])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "rating = data['movie', 'rates', 'reviewer'].rating\n",
    "\n",
    "train_edge_index = torch.arange(int(0.6 * len(rating)))\n",
    "val_edge_index = torch.arange(int(0.6 * len(rating)), int(0.9 * len(rating)))\n",
    "test_edge_index = torch.arange(int(0.9 * len(rating)), len(rating))\n",
    "\n",
    "train_labels = data['movie', 'rates', 'reviewer'].rating[train_edge_index].float().to(device)\n",
    "val_labels = data['movie', 'rates', 'reviewer'].rating[val_edge_index].float().to(device)\n",
    "test_labels = data['movie', 'rates', 'reviewer'].rating[test_edge_index].float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.6642, Train Acc: 0.6565, Val Loss: 0.6655, Val Acc: 0.6558\n",
      "Epoch 2/30, Train Loss: 0.6459, Train Acc: 0.6678, Val Loss: 0.6505, Val Acc: 0.6666\n",
      "Epoch 3/30, Train Loss: 0.6518, Train Acc: 0.6676, Val Loss: 0.6581, Val Acc: 0.6664\n",
      "Epoch 4/30, Train Loss: 0.6406, Train Acc: 0.6678, Val Loss: 0.6497, Val Acc: 0.6666\n",
      "Epoch 5/30, Train Loss: 0.6453, Train Acc: 0.6678, Val Loss: 0.6551, Val Acc: 0.6666\n",
      "Epoch 6/30, Train Loss: 0.6392, Train Acc: 0.6678, Val Loss: 0.6489, Val Acc: 0.6666\n",
      "Epoch 7/30, Train Loss: 0.6520, Train Acc: 0.6678, Val Loss: 0.6624, Val Acc: 0.6666\n",
      "Epoch 8/30, Train Loss: 0.6513, Train Acc: 0.6678, Val Loss: 0.6617, Val Acc: 0.6666\n",
      "Epoch 9/30, Train Loss: 0.6377, Train Acc: 0.6678, Val Loss: 0.6469, Val Acc: 0.6666\n",
      "Epoch 10/30, Train Loss: 0.6450, Train Acc: 0.6678, Val Loss: 0.6543, Val Acc: 0.6666\n",
      "Epoch 11/30, Train Loss: 0.6410, Train Acc: 0.6678, Val Loss: 0.6494, Val Acc: 0.6666\n",
      "Epoch 12/30, Train Loss: 0.6375, Train Acc: 0.6678, Val Loss: 0.6440, Val Acc: 0.6666\n",
      "Epoch 13/30, Train Loss: 0.6356, Train Acc: 0.6678, Val Loss: 0.6413, Val Acc: 0.6666\n",
      "Epoch 14/30, Train Loss: 0.6346, Train Acc: 0.6678, Val Loss: 0.6395, Val Acc: 0.6666\n",
      "Epoch 15/30, Train Loss: 0.6401, Train Acc: 0.6678, Val Loss: 0.6433, Val Acc: 0.6666\n",
      "Epoch 16/30, Train Loss: 0.6345, Train Acc: 0.6678, Val Loss: 0.6364, Val Acc: 0.6666\n",
      "Epoch 17/30, Train Loss: 0.6345, Train Acc: 0.6678, Val Loss: 0.6364, Val Acc: 0.6666\n",
      "Epoch 18/30, Train Loss: 0.6397, Train Acc: 0.6678, Val Loss: 0.6409, Val Acc: 0.6666\n",
      "Epoch 19/30, Train Loss: 0.6341, Train Acc: 0.6678, Val Loss: 0.6354, Val Acc: 0.6666\n",
      "Epoch 20/30, Train Loss: 0.6336, Train Acc: 0.6678, Val Loss: 0.6348, Val Acc: 0.6666\n",
      "Epoch 21/30, Train Loss: 0.6357, Train Acc: 0.6678, Val Loss: 0.6367, Val Acc: 0.6666\n",
      "Epoch 22/30, Train Loss: 0.6324, Train Acc: 0.6678, Val Loss: 0.6336, Val Acc: 0.6666\n",
      "Epoch 23/30, Train Loss: 0.6321, Train Acc: 0.6678, Val Loss: 0.6334, Val Acc: 0.6666\n",
      "Epoch 24/30, Train Loss: 0.6323, Train Acc: 0.6678, Val Loss: 0.6334, Val Acc: 0.6666\n",
      "Epoch 25/30, Train Loss: 0.6322, Train Acc: 0.6678, Val Loss: 0.6334, Val Acc: 0.6666\n",
      "Epoch 26/30, Train Loss: 0.6323, Train Acc: 0.6678, Val Loss: 0.6335, Val Acc: 0.6666\n",
      "Epoch 27/30, Train Loss: 0.6314, Train Acc: 0.6678, Val Loss: 0.6324, Val Acc: 0.6666\n",
      "Epoch 28/30, Train Loss: 0.6322, Train Acc: 0.6678, Val Loss: 0.6333, Val Acc: 0.6666\n",
      "Epoch 29/30, Train Loss: 0.6319, Train Acc: 0.6678, Val Loss: 0.6329, Val Acc: 0.6666\n",
      "Epoch 30/30, Train Loss: 0.6308, Train Acc: 0.6678, Val Loss: 0.6317, Val Acc: 0.6666\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "lr = 0.001\n",
    "\n",
    "model = HeteroGraphSAGE(\n",
    "        metadata=data.metadata(),  # HeteroData의 메타데이터 사용\n",
    "        hidden_channels=128,\n",
    "        out_channels=64\n",
    "    ).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    pred = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    # Train loss 계산\n",
    "    train_pred = pred[train_edge_index]\n",
    "    loss = loss_fn(train_pred, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss 계산\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = pred[val_edge_index]\n",
    "        val_loss = loss_fn(val_pred, val_labels)\n",
    "\n",
    "    # Accuracy 계산\n",
    "    train_acc = ((train_pred > 0.5).float() == train_labels).float().mean().item()\n",
    "    val_acc = ((val_pred > 0.5).float() == val_labels).float().mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6585\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data = data.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    pred = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    # 테스트 정확도 계산\n",
    "    test_pred = pred[test_edge_index]\n",
    "    test_acc = ((test_pred > 0.5).float() == test_labels).float().mean().item()\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
